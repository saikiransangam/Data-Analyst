{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "In order to get comfortable with common feature engineering techniques and how to implement them in python, we will use a toy dataset.  First, let's create the dataset, and we can also read in the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm;\n",
    "import sklearn.preprocessing as p\n",
    "\n",
    "df = pd.DataFrame({'response': [2.4, 3.3, -4.2, 5.6, 1.5, 8.7], \n",
    "                         'x1': ['yes','no','yes','maybe','no','yes'],\n",
    "                         'x2': [-1,-3,np.nan, 0, np.nan, 1],\n",
    "                         'x3': [2.4, 15, 3.3, 2.4, 1.8, 0.4],\n",
    "                         'x4': [np.nan, np.nan, 1, 1, 1, 1],\n",
    "                         'x5': ['A', 'B', np.nan, 'A', 'A', 'A']})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Fit a linear model between the response and the three x-variables in the dataset.  Also add an intercept.  Use the results to answer the first quiz question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['intercept'] = 1\n",
    "lm = sm.OLS(df['response'], df[['intercept','x2','x3','x4']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Use the sklearn documetation [here](http://scikit-learn.org/stable/modules/preprocessing.html) and the previous video to assist in filling in the missing values for each of the quantitative columns with the column mean.  Now, use the new columns to re-fit the linear model from question `1.`, and use the results to answer quiz 2 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = p.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(df[['x2', 'x3', 'x4']])\n",
    "df[['x2', 'x3', 'x4']] = imp.transform(df[['x2', 'x3', 'x4']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/statsmodels/stats/stattools.py:72: ValueWarning: omni_normtest is not valid with less than 8 observations; 6 samples were given.\n",
      "  \"samples were given.\" % int(n), ValueWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>response</td>     <th>  R-squared:         </th> <td>   0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 24 Nov 2017</td> <th>  Prob (F-statistic):</th>  <td> 0.365</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:57:07</td>     <th>  Log-Likelihood:    </th> <td> -14.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     6</td>      <th>  AIC:               </th> <td>   35.48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     3</td>      <th>  BIC:               </th> <td>   34.86</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>    1.1507</td> <td>    1.111</td> <td>    1.036</td> <td> 0.376</td> <td>   -2.384</td> <td>    4.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>    5.1202</td> <td>    3.053</td> <td>    1.677</td> <td> 0.192</td> <td>   -4.594</td> <td>   14.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>        <td>    1.0487</td> <td>    0.752</td> <td>    1.394</td> <td> 0.258</td> <td>   -1.345</td> <td>    3.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>        <td>    1.1507</td> <td>    1.111</td> <td>    1.036</td> <td> 0.376</td> <td>   -2.384</td> <td>    4.685</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>   nan</td> <th>  Durbin-Watson:     </th> <td>   2.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td>   nan</td> <th>  Jarque-Bera (JB):  </th> <td>   2.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.531</td> <th>  Prob(JB):          </th> <td>   0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.828</td> <th>  Cond. No.          </th> <td>4.21e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               response   R-squared:                       0.489\n",
       "Model:                            OLS   Adj. R-squared:                  0.149\n",
       "Method:                 Least Squares   F-statistic:                     1.438\n",
       "Date:                Fri, 24 Nov 2017   Prob (F-statistic):              0.365\n",
       "Time:                        21:57:07   Log-Likelihood:                -14.742\n",
       "No. Observations:                   6   AIC:                             35.48\n",
       "Df Residuals:                       3   BIC:                             34.86\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      1.1507      1.111      1.036      0.376      -2.384       4.685\n",
       "x2             5.1202      3.053      1.677      0.192      -4.594      14.835\n",
       "x3             1.0487      0.752      1.394      0.258      -1.345       3.442\n",
       "x4             1.1507      1.111      1.036      0.376      -2.384       4.685\n",
       "==============================================================================\n",
       "Omnibus:                          nan   Durbin-Watson:                   2.043\n",
       "Prob(Omnibus):                    nan   Jarque-Bera (JB):                2.515\n",
       "Skew:                          -1.531   Prob(JB):                        0.284\n",
       "Kurtosis:                       3.828   Cond. No.                     4.21e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.5e-31. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = sm.OLS(df['response'], df[['intercept','x2','x3','x4']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Another common way to scale features is by subtracting the mean and dividing by the standard deviation.  There are certain machine learning algorithms where you should always consider this type of scaling (or other ways of normalizing), as discussed [here](https://stats.stackexchange.com/questions/189652/is-it-a-good-practice-to-always-scale-normalize-data-for-machine-learning).  Use the [sklearn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) and the previous video to assist with performing this scaling on the three new, quantitative columns in your dataset.  \n",
    "\n",
    "To assure you performed these transformations correctly, answer quiz 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20701967, -0.3706604 ,  0.        ],\n",
       "       [-1.86317701,  2.20015852,  0.        ],\n",
       "       [ 0.        , -0.18703048,  0.        ],\n",
       "       [ 0.621059  , -0.3706604 ,  0.        ],\n",
       "       [ 0.        , -0.49308035,  0.        ],\n",
       "       [ 1.44913767, -0.7787269 ,  0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = p.StandardScaler()\n",
    "norm.fit(df[['x2','x3','x4']])\n",
    "norm.transform(df[['x2','x3','x4']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
